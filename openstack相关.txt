



openstack数据库:
root密码:
00000000
keystone/glance/nova/nova_api/nova_cell0/neutron帐号密码:
88888888

rabbitmqctl add_user openstack 99999999

keystone-manage bootstrap --bootstrap-password 77777777 --bootstrap-admin-url http://controller1:35357/v3/ --bootstrap-internal-url http://controller1:5000/v3/ --bootstrap-public-url http://controller1:5000/v3/ --bootstrap-region-id RegionOne

export OS_USERNAME=admin
export OS_PASSWORD=77777777
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3

openstack user create --domain default --password-prompt demo:
密码为:	66666666
openstack user create --domain default --password-prompt glance:
密码为:	66666666
openstack user create --domain default --password-prompt nova:
密码为:	66666666
openstack user create --domain default --password-prompt placement:
密码为:	66666666





export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=77777777
export OS_AUTH_URL=http://controller1:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2

export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=66666666
export OS_AUTH_URL=http://controller1:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2


[database]
connection = mysql+pymysql://glance:88888888@controller1/glance







MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY '88888888';



------------------------------------------------所有节点---------------------------------------------------

所有节点修改:
/etc/hosts:
192.168.8.11	controller1
192.168.8.12	computer1
192.168.8.13	block1
192.168.8.14	object1

Controller node:

1. Install the packages:
# yum install chrony
2./etc/chrony.conf:
server NTP_SERVER iburst
allow 192.168.8.0/24
3.启动:
# systemctl enable chronyd.service
# systemctl start chronyd.service

Other nodes:

1. Install the packages:
# yum install chrony
2./etc/chrony.conf:
server controller1 iburst
3.启动:
# systemctl enable chronyd.service
# systemctl start chronyd.service

Verify operation:

1. Run this command on the controller node:
# chronyc sources
2. Run the same command on all other nodes:
# chronyc sources

所有节点安装:
# yum install centos-release-openstack-queens
# yum upgrade
# yum install python-openstackclient
# yum install openstack-selinux



------------------------------------------------控制节点---------------------------------------------------

安装SQL DATABASE:

1.Install the packages:
# yum install mariadb mariadb-server python2-PyMySQL
2.Create and edit the /etc/my.cnf.d/openstack.cnf:
[mysqld]
bind-address = 192.168.8.11
default-storage-engine = innodb
innodb_file_per_table = on
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8
3.启动:
# systemctl enable mariadb.service
# systemctl start mariadb.service
4.执行脚本进行安全设置并设置数据库root密码:
# mysql_secure_installation
00000000

安装Message queue:
1. Install the package:
# yum install rabbitmq-server
2.启动:
# systemctl enable rabbitmq-server.service
# systemctl start rabbitmq-server.service
3.Add the openstack user:
# rabbitmqctl add_user openstack 99999999
4.Permit configuration,write,and read access for the openstack user:
# rabbitmqctl set_permissions openstack ".*" ".*" ".*"

安装Memcached:
1. Install the packages:
# yum install memcached python-memcached
2.修改/etc/sysconfig/memcached
OPTIONS="-l 127.0.0.1,::1,controller1"
3.启动:
# systemctl enable memcached.service
# systemctl start memcached.service

安装Identity service:

Prerequisites:

# mysql -u root -p
MariaDB [(none)]> CREATE DATABASE keystone;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY '88888888';

1.安装包:
# yum install openstack-keystone httpd mod_wsgi
2.修改/etc/keystone/keystone.conf:
[database]
connection = mysql+pymysql://keystone:88888888@controller1/keystone
[token]
provider = fernet
3.Populate the Identity service database:
# su -s /bin/sh -c "keystone-manage db_sync" keystone
4.Initialize Fernet key repositories:
# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
5.Bootstrap the Identity service:
# keystone-manage bootstrap --bootstrap-password 77777777 --bootstrap-admin-url http://controller1:35357/v3/ --bootstrap-internal-url http://controller1:5000/v3/ --bootstrap-public-url http://controller1:5000/v3/ --bootstrap-region-id RegionOne
6.修改/etc/httpd/conf/httpd.conf:
ServerName controller1
7.Create a link:
# ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
8.启动httpd:
# systemctl enable httpd.service
# systemctl start httpd.service

9.执行以下命令Configure the administrative account:
export OS_USERNAME=admin
export OS_PASSWORD=77777777
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller1:35357/v3
export OS_IDENTITY_API_VERSION=3

10.Create a domain, projects, users, and roles:
The Identity service provides authentication services for each OpenStack service. The authentication service uses a combination of domains, projects, users, and roles.
This guide uses a service project that contains a unique user for each service that you add to your environment.

10.1)Create the service project:

#openstack project create --domain default --description "Service Project" service

Regular(non-admin) tasks should use an unprivileged project and user.As an example,this guide creates the demo project and user.

10.2)Create the demo project:
#openstack project create --domain default --description "Demo Project" demo

10.3)Create the demo user:
#openstack user create --domain default --password-prompt demo
输入密码:	66666666

10.4)Create the user role:
#openstack role create user

10.5)Add the user role to the demo project and user:
#openstack role add --project demo --user demo user

11.Verify operation:
Verify operation of the Identity service before installing other services.

1)For security reasons, disable the temporary authentication token mechanism:
Edit the /etc/keystone/keystone-paste.ini file and remove admin_token_auth from the
[pipeline:public_api], [pipeline:admin_api], and [pipeline:api_v3] sections.

2)Unset the temporary OS_AUTH_URL and OS_PASSWORD environment variable:
#unset OS_AUTH_URL OS_PASSWORD

3)As the admin user,request an authentication token:
#openstack --os-auth-url http://controller1:35357/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name admin --os-username admin token issue
输入admin的密码:77777777

4)As the demo user,request an authentication token:
#openstack --os-auth-url http://controller1:5000/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name demo --os-username demo token issue
输入demon的密码:66666666


5)Create OpenStack client environment scripts:

5.1)Create and edit /etc/admin-openrc file and add the following content:
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=77777777
export OS_AUTH_URL=http://controller1:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2

5.2)Create and edit /etc/demo-openrc file and add the following content:
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=66666666
export OS_AUTH_URL=http://controller1:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2

5.3)Using the scripts:
#. /etc/admin-openrc
#openstack token issue

安装Image service:

The Image service (glance) enables users to discover, register, and retrieve virtual machine images. It offers a REST API that enables you to query virtual machine image metadata and retrieve an actual image. You can store virtual machine images made available through the Image service in a variety of locations, from simple file systems to object-storage systems like OpenStack Object Storage.

For simplicity,this guide describes configuring the Image service to use the file backend,which uploads and stores in a directory on the controller node hosting the Image service. By default, this directory is /var/lib/glance/images/.

Prerequisites:

1.数据库操作:
# mysql -u root -p
MariaDB [(none)]> CREATE DATABASE glance;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY '88888888';

2.应用环境变量:
#. /etc/admin-openrc

3.create the service credentials:

3.1)Create the glance user:
#openstack user create --domain default --password-prompt glance
输入密码:	66666666

3.2)Add the admin role to the glance user and service project:
#openstack role add --project service --user glance admin


3.3)Create the glance service entity:
#openstack service create --name glance --description "OpenStack Image" image

4.Create the Image service API endpoints:
#openstack endpoint create --region RegionOne image public http://controller1:9292
#openstack endpoint create --region RegionOne image internal http://controller1:9292
#openstack endpoint create --region RegionOne image admin http://controller1:9292

Install and configure components:
正式安装Image service:

1. Install the packages:
# yum install openstack-glance

2.编辑/etc/glance/glance-api.conf:

[database]
connection = mysql+pymysql://glance:88888888@controller1/glance

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = 66666666

[paste_deploy]
flavor = keystone

[glance_store]
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

3.编辑/etc/glance/glance-registry.conf:

[database]
connection = mysql+pymysql://glance:88888888@controller1/glance

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = 66666666

[paste_deploy]
flavor = keystone

4.Populate the Image service database:
#su -s /bin/sh -c "glance-manage db_sync" glance

5.启动:
# systemctl enable openstack-glance-api.service openstack-glance-registry.service
# systemctl start openstack-glance-api.service openstack-glance-registry.service

6.Verify operation:

6.1)Source the admin credentials to gain access to admin-only CLI commands:
#. /etc/admin-openrc

6.2)Download the source image:
#wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img

6.3)Upload the image to the Image service using the QCOW2 disk format,bare container format,and public
visibility so all projects can access it:
#openstack image create "cirros" --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public

6.4)Confirm upload of the image and validate attributes:
#openstack image list


安装Compute service:

Prerequisites:

1.数据库操作:
# mysql -u root -p
MariaDB [(none)]> CREATE DATABASE nova_api;
MariaDB [(none)]> CREATE DATABASE nova;
MariaDB [(none)]> CREATE DATABASE nova_cell0;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY '88888888';

2.应用环境变量:
#. /etc/admin-openrc

3.Create the Compute service credentials:

3.1)Create the nova user:
#openstack user create --domain default --password-prompt nova
输入密码:	66666666

3.2)Add the admin role to the nova user and service project:
#openstack role add --project service --user nova admin

3.3)Create the nova service entity:
#openstack service create --name nova --description "OpenStack Compute" compute

4.Create the Compute API service endpoints:
#openstack endpoint create --region RegionOne compute public http://controller1:8774/v2.1
#openstack endpoint create --region RegionOne compute internal http://controller1:8774/v2.1
#openstack endpoint create --region RegionOne compute admin http://controller1:8774/v2.1

5.Create a Placement service user using your chosen PLACEMENT_PASS:
#openstack user create --domain default --password-prompt placement
输入密码:	66666666

6.Add the Placement user to the service project with the admin role:
#openstack role add --project service --user placement admin

7.Create the Placement API entry in the service catalog:
#openstack service create --name placement --description "Placement API" placement

8.Create the Placement API service endpoints:
#openstack endpoint create --region RegionOne placement public http://controller1/placement
 openstack endpoint create --region RegionOne placement public http://controller1:8778
#openstack endpoint create --region RegionOne placement internal http://controller1/placement
openstack endpoint create --region RegionOne placement internal http://controller1:8778
#openstack endpoint create --region RegionOne placement admin http://controller1/placement
openstack endpoint create --region RegionOne placement admin http://controller1:8778

Install and configure components
正式安装Compute service:

1、Install the packages:

#yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api
   
2、修改/etc/nova/nova.conf:
Note: Comment out or remove any other options in the [keystone_authtoken] section.

[DEFAULT]
enabled_apis = osapi_compute,metadata

[api_database]
connection = mysql+pymysql://nova:88888888@controller1/nova_api

[database]
connection = mysql+pymysql://nova:88888888@controller1/nova

[DEFAULT]
transport_url = rabbit://openstack:99999999@controller1

[api]
auth_strategy = keystone
 
[keystone_authtoken]
auth_url = http://controller1:5000/v3
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = 66666666
 
[DEFAULT]
my_ip = 192.168.8.11

[DEFAULT]
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[vnc]
enabled = true

server_listen = $my_ip
server_proxyclient_address = $my_ip

[glance]
api_servers = http://controller1:9292   

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[placement]
os_region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller1:5000/v3
username = placement
password = 66666666
 
3、修改/etc/httpd/conf.d/00-nova-placement-api.conf:

<Directory /usr/bin>
   <IfVersion >= 2.4>
      Require all granted
   </IfVersion>
   <IfVersion < 2.4>
      Order allow,deny
      Allow from all
   </IfVersion>
</Directory>

# systemctl restart httpd

4、Populate the nova-api database:

# su -s /bin/sh -c "nova-manage api_db sync" nova

5、Registerthecell0database:

#su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova

6、Createthecell1cell:

 # su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova
 
7、Populate the nova database:

# su -s /bin/sh -c "nova-manage db sync" nova

8、Verify nova cell0 and cell1 are registered correctly:

# nova-manage cell_v2 list_cells

9、Start the Compute services and configure them to start when the system boots:

# systemctl enable openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
# systemctl start openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service


------------------------------------------------计算节点---------------------------------------------------




1. Install the packages:
# yum install openstack-nova-compute

2. Edit the /etc/nova/nova.conf:
Note: Comment out or remove any other options in the [keystone_authtoken] section.

[DEFAULT]
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:99999999@controller1
my_ip = 192.168.8.12
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[api]
auth_strategy = keystone

[keystone_authtoken]
auth_url = http://controller1:5000/v3
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = 66666666

[vnc]
enabled = True
server_listen = 0.0.0.0
server_proxyclient_address = $my_ip
novncproxy_base_url = http://controller1:6080/vnc_auto.html

[glance]
api_servers = http://controller1:9292
     
[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[placement]
os_region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller1:5000/v3
username = placement
password = 66666666


3.Finalize installation:

3.1)Determine whether your compute node supports hardware acceleration for virtual machines:

#egrep -c '(vmx|svm)' /proc/cpuinfo

If this command returns a value of one or greater, your compute node supports hardware acceleration
which typically requires no additional configuration.

If this command returns a value of zero, your compute node does not support hardware acceleration and
you must configure libvirt to use QEMU instead of KVM.

	Edit the [libvirt] section in the /etc/nova/nova.conf file as follows:

	[libvirt]
	virt_type = qemu
	
3.2)Start the Compute service including its dependencies and configure them to start automatically when the system boots:

#systemctl enable libvirtd.service openstack-nova-compute.service 
#systemctl start libvirtd.service openstack-nova-compute.service
 
Note: If the nova-compute service fails to start, check /var/log/nova/nova-compute.log.

4.Add the compute node to the cell database:

*************Important: Run the following commands on the controller node.************

4.1)Source the admin credentials to enable admin-only CLI commands,then confirm there are compute hosts in the database:

#. /etc/admin-openrc
#openstack compute service list --service nova-compute

4.2)Discover compute hosts:
# su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova

Note: When you add new compute nodes, you must run nova-manage cell_v2 discover_hosts on the controller node to register those new compute nodes. Alternatively, you can set an appropriate interval in /etc/nova/nova.conf:
[scheduler]
discover_hosts_in_cells_interval = 300

**************************************************************************************

5.Verify operation:

*************Note: Perform these commands on the controller node.*********************

#. /etc/admin-openrc

#openstack compute service list

#openstack catalog list

#openstack image list

#nova-status upgrade check

**************************************************************************************



------------------------------------------------网络配置(neutron)---------------------------------------------------

*************Note: Perform these commands on the controller node.*********************

Install and configure controller node:

Prerequisites:

1.数据库操作:
# mysql -u root -p
MariaDB [(none)]> CREATE DATABASE neutron;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY '88888888';

2.应用环境变量:
#. /etc/admin-openrc

3.Create the Network service credentials:

3.1)Create the neutron user:
#openstack user create --domain default --password-prompt neutron
输入密码:	66666666

3.2)Add the admin role to the neutron user and service project:
#openstack role add --project service --user neutron admin

3.3)Create the neutron service entity:
#openstack service create --name neutron --description "OpenStack Networking" network

4.Create the Networking service API endpoints:
#openstack endpoint create --region RegionOne network public http://controller1:9696
#openstack endpoint create --region RegionOne network internal http://controller1:9696
#openstack endpoint create --region RegionOne network admin http://controller1:9696


Configure networking options:

You can deploy the Networking service using one of two architectures represented by options 1 and 2.
Option 1 deploys the simplest possible architecture that only supports attaching instances to provider (exter- nal) networks. No self-service (private) networks, routers, or floating IP addresses. Only the admin or other privileged user can manage provider networks.
Option 2 augments option 1 with layer-3 services that support attaching instances to self-service networks. The demo or other unprivileged user can manage self-service networks including routers that provide connectiv- ity between self-service and provider networks. Additionally, floating IP addresses provide connectivity to instances using self-service networks from external networks such as the Internet.
Self-service networks typically use overlay networks. Overlay network protocols such as VXLAN include additional headers that increase overhead and decrease space available for the payload or user data. Without knowledge of the virtual network infrastructure, instances attempt to send packets using the default Ethernet maximum transmission unit (MTU) of 1500 bytes. The Networking service automatically provides the correct MTU value to instances via DHCP. However, some cloud images do not use DHCP or ignore the DHCP MTU option and require configuration using metadata or a script.

Note: Option 2 also supports attaching instances to provider networks.

Choose one of the following networking options to configure services specific to it. Afterwards, return here and proceed to Configure the metadata agent.

----------

一种情况:
Networking Option 1: Provider networks

Install and configure the Networking components on the controller node:

1.Install the components:
#yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables

2.编辑/etc/neutron/neutron.conf(Configure the server component):
Note: Comment out or remove any other connection options in the [database] section.
Note: Comment out or remove any other options in the [keystone_authtoken] section.

[database]
connection = mysql+pymysql://neutron:88888888@controller1/neutron

[DEFAULT]
core_plugin = ml2
service_plugins =
transport_url = rabbit://openstack:99999999@controller1
auth_strategy = keystone
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = 66666666

[nova]
auth_url = http://controller1:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = 66666666

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp

3.编辑/etc/neutron/plugins/ml2/ml2_conf.ini(Configure the Modular Layer 2 (ML2) plug-in):

In the [ml2] section,enable flat and VLAN networks,disable self-service networks,enable the Linux bridge mechanism,enable the port security extension driver:

[ml2]
type_drivers = flat,vlan
tenant_network_types =
mechanism_drivers = linuxbridge
extension_drivers = port_security
    
[ml2_type_flat]
flat_networks = provider

[securitygroup]
enable_ipset = true


4.编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini(Configure the Linux bridge agent):

[linux_bridge]
physical_interface_mappings = provider:enp0s3

[vxlan]
enable_vxlan = false

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

5.编辑/etc/neutron/dhcp_agent.ini(Configure the DHCP agent):

[DEFAULT]
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true

然后,Return to Networking controller node configuration.

----------

----------

另一种情况:
Networking Option 2: Self-service networks

Install and configure the Networking components on the controller node:

1.Install the components:
#yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables

2.编辑/etc/neutron/neutron.conf(Configure the server component):

Note: Comment out or remove any other connection options in the [database] section.
Note: Comment out or remove any other options in the [keystone_authtoken] section.

[database]
connection = mysql+pymysql://neutron:88888888@controller1/neutron

[DEFAULT]
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = true
transport_url = rabbit://openstack:99999999@controller1
auth_strategy = keystone
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = 66666666

[nova]
auth_url = http://controller1:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = 66666666

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp

3.编辑/etc/neutron/plugins/ml2/ml2_conf.ini(Configure the Modular Layer 2 (ML2) plug-in):

Note: The Linux bridge agent only supports VXLAN overlay networks.

[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = linuxbridge,l2population
extension_drivers = port_security

[ml2_type_flat]
flat_networks = provider

[ml2_type_vxlan]
vni_ranges = 1:1000

[securitygroup]
enable_ipset = true

***4.编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini(Configure the Linux bridge agent):

[linux_bridge]
physical_interface_mappings = provider:enp0s3

[vxlan]
enable_vxlan = true
local_ip = 192.168.8.11
l2_population = true

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

备注:
Ensure your Linux operating system kernel supports network bridge filters by verifying all the following sysctl values are set to 1:

net.bridge.bridge-nf-call-iptables
net.bridge.bridge-nf-call-ip6tables

***5.编辑/etc/neutron/l3_agent.ini(Configure the layer-3 agent):

[DEFAULT]
interface_driver = linuxbridge

***6.编辑/etc/neutron/dhcp_agent.ini(Configure the DHCP agent):

[DEFAULT]
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true

然后,Return to Networking controller node configuration.

----------

选项1与选项2的公共执行部分:

***1.编辑/etc/neutron/metadata_agent.ini(Configure the metadata agent):

[DEFAULT]
nova_metadata_ip = controller1
metadata_proxy_shared_secret = 33333333

2.编辑/etc/nova/nova.conf(Configure the Compute service to use the Networking service):

[neutron]
url = http://controller1:9696
auth_url = http://controller1:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = 66666666
service_metadata_proxy = true
metadata_proxy_shared_secret = 33333333


Finalize installation:

依次执行:

# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini

# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

# systemctl restart openstack-nova-api.service

# systemctl enable neutron-server.service 
***# systemctl enable neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
# systemctl start neutron-server.service
***# systemctl start neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service

For networking option 2, also enable and start the layer-3 service:
***# systemctl enable neutron-l3-agent.service 
***# systemctl start neutron-l3-agent.service


***************************************************************************************

备注:
	配置独立网络节点:
	备注: Three network interfaces: management, overlay, and provider
	
	在网络节点上执行以下步骤:
	
	1.安装:
	#yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables

	2.编辑/etc/neutron/neutron.conf:

	Note: Comment out or remove any other connection options in the [database] section.
	Note: Comment out or remove any other options in the [keystone_authtoken] section.

	[DEFAULT]
	core_plugin = ml2
	service_plugins = router
	allow_overlapping_ips = true
	transport_url = rabbit://openstack:99999999@controller1
	auth_strategy = keystone
	notify_nova_on_port_status_changes = true
	notify_nova_on_port_data_changes = true

	[oslo_concurrency]
	lock_path = /var/lib/neutron/tmp

	3.将上面控制节点中***开始的指令移动到网络节点



*************Note: Perform these commands on the compute node.*********************


1.Install the components:

# yum install openstack-neutron-linuxbridge ebtables ipset

2.编辑/etc/neutron/neutron.conf(Configure the common component):

In the [database] section,comment out any connection options because compute nodes do not directly access the database.
Note: Comment out or remove any other options in the [keystone_authtoken] section.


[DEFAULT]
transport_url = rabbit://openstack:99999999@controller1
auth_strategy = keystone

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = 66666666

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp

3.Configure networking options

注意:Choose the same networking option that you chose for the controller node to configure services specific to it. 

一种情况:
Networking Option 1: Provider networks

编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini(Configure the Linux bridge agent):

[linux_bridge]
physical_interface_mappings = provider:enp0s3

[vxlan]
enable_vxlan = false

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver


另一种情况:
Networking Option 2: Self-service networks

编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini(Configure the Linux bridge agent):

[linux_bridge]
physical_interface_mappings = provider:enp0s3

[vxlan]
enable_vxlan = true
local_ip = 192.168.8.12
l2_population = true

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver


4.编辑/etc/nova/nova.conf(Configure the Compute service to use the Networking service):

[neutron]
url = http://controller1:9696
auth_url = http://controller1:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = 66666666


5.Finalize installation:

依次执行:

# systemctl restart openstack-nova-compute.service
# systemctl enable neutron-linuxbridge-agent.service
# systemctl start neutron-linuxbridge-agent.service


6.Verify operation:

Note: Perform these commands on the controller node.
在控制节点执行验证:

#. /etc/admin-openrc

#openstack extension list --network

#openstack network agent list
说明:
Networking Option 1: Provider networks:
The output should indicate three agents on the controller node and one agent on each compute node.
Networking Option 2: Self-service networks:
The output should indicate four agents on the controller node and one agent on each compute node.



************************************************************************************


--------------------------------------------Dashboard(horizon)-------------------------------------------------


This section describes how to install and configure the dashboard on the controller node.


1.Install the packages:
# yum install openstack-dashboard

2.Edit the /etc/openstack-dashboard/local_settings:

Note:Comment out any other session storage configuration.

OPENSTACK_HOST = "controller1"
ALLOWED_HOSTS = ['*']
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': 'controller1:11211',
    }
}

OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 2,
}

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"

OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

If you chose networking option 1, disable support for layer-3 networking services:
OPENSTACK_NEUTRON_NETWORK = {
    ...
    'enable_router': False,
    'enable_quotas': False,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_lb': False,
    'enable_firewall': False,
    'enable_vpn': False,
    'enable_fip_topology_check': False,
}

TIME_ZONE = "Asia/Chongqing"

3.Finalize installation:

# systemctl restart httpd.service memcached.service

4.Verify operation:

Access the dashboard using a web browser at http://controller1/dashboard. 
Authenticate using admin or demo user and default domain credentials.


--------------------------------------------Block Storage service(cinder)-------------------------------------------------

************Note: Perform these commands on the controller node.*********************

This section describes how to install and configure the Block Storage service, code-named cinder, on the controller node. This service requires at least one additional storage node that provides volumes to instances.

Install and configure controller node:

1.create the database:

$ mysql -u root -p

MariaDB [(none)]> CREATE DATABASE cinder;

MariaDB [(none)]> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
  IDENTIFIED BY '88888888';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
  IDENTIFIED BY '88888888';
  
2.To create the service credentials, complete these steps:
  
$ . /etc/admin-openrc

$ openstack user create --domain default --password-prompt cinder
输入密码:	66666666

$ openstack role add --project service --user cinder admin
(Add the admin role to the cinder user)

Create the cinderv2 and cinderv3 service entities:
Note: The Block Storage services require two service entities.

$ openstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2
$ openstack service create --name cinderv3 --description "OpenStack Block Storage" volumev3

旧版:
$ openstack service create --name cinder --description "OpenStack Block Storage" volume
$ openstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2

3.Create the Block Storage service API endpoints:
Note: The Block Storage services require endpoints for each service entity.

$ openstack endpoint create --region RegionOne volumev2 public http://controller1:8776/v2/%\(project_id\)s
$ openstack endpoint create --region RegionOne volumev2 internal http://controller1:8776/v2/%\(project_id\)s
$ openstack endpoint create --region RegionOne volumev2 admin http://controller1:8776/v2/%\(project_id\)s

$ openstack endpoint create --region RegionOne volumev3 public http://controller1:8776/v3/%\(project_id\)s
$ openstack endpoint create --region RegionOne volumev3 internal http://controller1:8776/v3/%\(project_id\)s
$ openstack endpoint create --region RegionOne volumev3 admin http://controller1:8776/v3/%\(project_id\)s


旧版:
$ openstack endpoint create --region RegionOne volume public http://controller1:8776/v1/%\(tenant_id\)s
$ openstack endpoint create --region RegionOne volume internal http://controller1:8776/v1/%\(tenant_id\)s
$ openstack endpoint create --region RegionOne volume admin http://controller1:8776/v1/%\(tenant_id\)s

$ openstack endpoint create --region RegionOne volumev2 public http://controller1:8776/v2/%\(tenant_id\)s
$ openstack endpoint create --region RegionOne volumev2 internal http://controller1:8776/v2/%\(tenant_id\)s
$ openstack endpoint create --region RegionOne volumev2 admin http://controller1:8776/v2/%\(tenant_id\)s

4.Install and configure components:

4.1)Install the packages:

# yum install openstack-cinder

4.2)Edit the /etc/cinder/cinder.conf:
Note: Comment out or remove any other options in the [keystone_authtoken] section.

[database]
connection = mysql+pymysql://cinder:88888888@controller1/cinder

[DEFAULT]
transport_url = rabbit://openstack:99999999@controller1
auth_strategy = keystone
my_ip = 192.168.8.11

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_id = default
user_domain_id = default
project_name = service
username = cinder
password = 66666666

[oslo_concurrency]
lock_path = /var/lib/cinder/tmp

4.3)Populate the Block Storage database:
Note: Ignore any deprecation messages in this output.

# su -s /bin/sh -c "cinder-manage db sync" cinder

5.Configure Compute to use Block Storage:

Edit the /etc/nova/nova.conf:

[cinder]
os_region_name = RegionOne

6.Finalize installation:

# systemctl restart openstack-nova-api.service
# systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
# systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service

*************************************************************************************

***************Note: Perform these commands on the storage node.*********************

This section describes how to install and configure storage nodes for the Block Storage service. For simplicity, this configuration references one storage node with an empty local block storage device. The instructions use /dev/sdb, but you can substitute a different value for your particular node.

***Note: Perform these steps on the storage node.***

Install and configure a storage node:

1.Install the supporting utility packages:

Note: Some distributions include LVM by default.

# yum install lvm2 device-mapper-persistent-data

旧版:
# yum install lvm2

# systemctl enable lvm2-lvmetad.service
# systemctl start lvm2-lvmetad.service

2.Create the LVM physical volume /dev/sdb:

# pvcreate /dev/sdb

3.Create the LVM volume group cinder-volumes:

# vgcreate cinder-volumes /dev/sdb

4.Only instances can access Block Storage volumes. However, the underlying operating system manages the devices associated with the volumes. By default, the LVM volume scanning tool scans the /dev directory for block storage devices that contain volumes. If projects use LVM on their volumes, the scanning tool detects these volumes and attempts to cache them which can cause a variety of problems with both the underlying operating system and project volumes. You must reconfigure LVM to scan only the devices that contain the cinder-volumes volume group. Edit the /etc/lvm/lvm.conf file and complete the following actions:

In the devices section, add a filter that accepts the /dev/sdb device and rejects all other devices:

devices {
...
filter = [ "a/sdb/", "r/.*/"]

}


Each item in the filter array begins with a for accept or r for reject and includes a regular expression for the device name. The array must end with r/.*/ to reject any remaining devices. You can use the vgs -vvvv command to test filters.

*******Warning*******

If your storage nodes use LVM on the operating system disk, you must also add the associated device to the filter. For example, if the /dev/sda device contains the operating system:

filter = [ "a/sda/", "a/sdb/", "r/.*/"]

Similarly, if your compute nodes use LVM on the operating system disk, you must also modify the filter in the /etc/lvm/lvm.conf file on those nodes to include only the operating system disk. For example, if the /dev/sda device contains the operating system:

filter = [ "a/sda/", "r/.*/"]

*********************

5.Install and configure components:

5.1)Install the packages:
# yum install openstack-cinder targetcli python-keystone

5.2)Edit the /etc/cinder/cinder.conf:

[database]
connection = mysql+pymysql://cinder:88888888@controller1/cinder

[DEFAULT]
transport_url = rabbit://openstack:99999999@controller1
auth_strategy = keystone
my_ip = 192.168.8.13
enabled_backends = lvm
glance_api_servers = http://controller1:9292

[keystone_authtoken]
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_id = default
user_domain_id = default
project_name = service
username = cinder
password = 66666666

[lvm]
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_group = cinder-volumes
iscsi_protocol = iscsi
iscsi_helper = lioadm

[oslo_concurrency]
lock_path = /var/lib/cinder/tmp

6.Finalize installation:

# systemctl enable openstack-cinder-volume.service target.service
# systemctl start openstack-cinder-volume.service target.service


7.Verify Cinder operation:

NOTE:
***Perform these commands on the controller node.***

$ . /etc/admin-openrc

$ openstack volume service list



8.Install and configure the backup service(可选):
Optionally, install and configure the backup service. For simplicity, this configuration uses the Block Storage node and the Object Storage (swift) driver, thus depending on the Object Storage service.

NOTE: You must install and configure a storage node prior to installing and configuring the backup service.

NOTE: Perform these steps on the Block Storage node:

8.1)安装:
# yum install openstack-cinder

8.2)Edit the /etc/cinder/cinder.conf:

[DEFAULT]
backup_driver = cinder.backup.drivers.swift
backup_swift_url = SWIFT_URL

备注:Replace SWIFT_URL with the URL of the Object Storage service. The URL can be found by showing the object-store API endpoints:
	$ openstack catalog show object-store


8.3)Finalize installation:

# systemctl enable openstack-cinder-backup.service
# systemctl start openstack-cinder-backup.service


--------------------------------------------Object Storage service(swift)-------------------------------------------------


************Note: Perform these commands on the controller node.*********************

Note: The Object Storage service does not use an SQL database on the controller node. Instead, it uses distributed SQLite databases on each storage node.

依次执行:

$ . /etc/admin-openrc

$ openstack user create --domain default --password-prompt swift
输入密码:	66666666

$ openstack role add --project service --user swift admin

$ openstack service create --name swift --description "OpenStack Object Storage" object-store

$ openstack endpoint create --region RegionOne object-store public http://controller1:8080/v1/AUTH_%\(project_id\)s

$ openstack endpoint create --region RegionOne object-store internal http://controller1:8080/v1/AUTH_%\(project_id\)s

$ openstack endpoint create --region RegionOne object-store admin http://controller1:8080/v1


# yum install openstack-swift-proxy python-swiftclient python-keystoneclient python-keystonemiddleware memcached

# curl -o /etc/swift/proxy-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/queens

Edit the /etc/swift/proxy-server.conf:
Note: Comment out or remove any other options in the [filter:authtoken] section.

[DEFAULT]
bind_port = 8080
user = swift
swift_dir = /etc/swift

[pipeline:main]
pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server

[app:proxy-server]
use = egg:swift
account_autocreate = True

[filter:keystoneauth]
use = egg:swift
operator_roles = admin,user

[filter:authtoken]
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_uri = http://controller1:5000
auth_url = http://controller1:35357
memcached_servers = controller1:11211
auth_type = password
project_domain_id = default
user_domain_id = default
project_name = service
username = swift
password = 66666666
delay_auth_decision = True

[filter:cache]
use = egg:swift
memcache_servers = controller1:11211

*********************************************************************************

***********Note: Perform these commands on the storage node.*********************

依次执行:

# yum install xfsprogs rsync

# mkfs.xfs /dev/sdb
# mkfs.xfs /dev/sdc
备注:假设使用/dev/sdb and /dev/sdc

# mkdir -p /srv/node/sdb
# mkdir -p /srv/node/sdc

Edit the /etc/fstab:

/dev/sdb /srv/node/sdb xfs noatime,nodiratime,nobarrier,logbufs=8 0 2
/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2

# mount /srv/node/sdb
# mount /srv/node/sdc

Create or edit the /etc/rsyncd.conf:

uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = 192.168.8.14

[account]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/account.lock

[container]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/container.lock

[object]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/object.lock

# systemctl enable rsyncd.service
# systemctl start rsyncd.service

# yum install openstack-swift-account openstack-swift-container openstack-swift-object

# curl -o /etc/swift/account-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/account-server.conf-sample?h=stable/queens
# curl -o /etc/swift/container-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/container-server.conf-sample?h=stable/queens
# curl -o /etc/swift/object-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/object-server.conf-sample?h=stable/queens

Edit the /etc/swift/account-server.conf:

[DEFAULT]
bind_ip = 192.168.8.14
bind_port = 6202
user = swift
swift_dir = /etc/swift
devices = /srv/node
mount_check = True

[pipeline:main]
pipeline = healthcheck recon account-server

[filter:recon]
use = egg:swift
recon_cache_path = /var/cache/swift

Edit the /etc/swift/container-server.conf:

[DEFAULT]
bind_ip = 192.168.8.14
bind_port = 6201
user = swift
swift_dir = /etc/swift
devices = /srv/node
mount_check = True

[pipeline:main]
pipeline = healthcheck recon container-server

[filter:recon]
use = egg:swift
recon_cache_path = /var/cache/swift

Edit the /etc/swift/object-server.conf:

[DEFAULT]
bind_ip = 192.168.8.14
bind_port = 6200
user = swift
swift_dir = /etc/swift
devices = /srv/node
mount_check = True

[pipeline:main]
pipeline = healthcheck recon object-server

[filter:recon]
use = egg:swift
recon_cache_path = /var/cache/swift
recon_lock_path = /var/lock

# chown -R swift:swift /srv/node

# mkdir -p /var/cache/swift
# chown -R root:swift /var/cache/swift
# chmod -R 775 /var/cache/swift

*************************************************************************************


************Note: Perform these commands on the controller node.*********************

Create and distribute initial rings:

Before starting the Object Storage services, you must create the initial account, container, and object rings. The ring builder creates configuration files that each node uses to determine and deploy the storage architecture. For simplicity, this guide uses one region and two zones with 2^10 (1024) maximum partitions, 3 replicas of each object, and 1 hour minimum time between moving a partition more than once. For Object Storage, a partition indicates a directory on a storage device rather than a conventional partition table.

Note: Perform these steps on the controller node.

#cd /etc/swift

# swift-ring-builder account.builder create 10 3 1

# swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.8.14 --port 6202 --device sdb --weight 100
# swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.8.14 --port 6202 --device sdc --weight 100

备注: swift-ring-builder account.builder add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6202 --device DEVICE_NAME --weight DEVICE_WEIGHT
备注: Replace STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS with the IP address of the management network on the storage node. Replace DEVICE_NAME with a storage device name on the same storage node.
备注示例:
	# swift-ring-builder account.builder add \
	  --region 1 --zone 1 --ip 10.0.0.51 --port 6202 --device sdb --weight 100
	Device d0r1z1-10.0.0.51:6202R10.0.0.51:6202/sdb_"" with 100.0 weight got id 0
	# swift-ring-builder account.builder add \
	  --region 1 --zone 1 --ip 10.0.0.51 --port 6202 --device sdc --weight 100
	Device d1r1z2-10.0.0.51:6202R10.0.0.51:6202/sdc_"" with 100.0 weight got id 1
	# swift-ring-builder account.builder add \
	  --region 1 --zone 2 --ip 10.0.0.52 --port 6202 --device sdb --weight 100
	Device d2r1z3-10.0.0.52:6202R10.0.0.52:6202/sdb_"" with 100.0 weight got id 2
	# swift-ring-builder account.builder add \
	  --region 1 --zone 2 --ip 10.0.0.52 --port 6202 --device sdc --weight 100
	Device d3r1z4-10.0.0.52:6202R10.0.0.52:6202/sdc_"" with 100.0 weight got id 3


Verify the ring contents:

# swift-ring-builder account.builder

Rebalance the ring:

# swift-ring-builder account.builder rebalance

cd /etc/swift

# swift-ring-builder container.builder create 10 3 1

# swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.8.14 --port 6201 --device sdb --weight 100
# swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.8.14 --port 6201 --device sdc --weight 100

备注: swift-ring-builder container.builder add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6201 --device DEVICE_NAME --weight DEVICE_WEIGHT
备注: Replace STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS with the IP address of the management network on the storage node. Replace DEVICE_NAME with a storage device name on the same storage node. 
备注示例:
	# swift-ring-builder container.builder add \
	  --region 1 --zone 1 --ip 10.0.0.51 --port 6201 --device sdb --weight 100
	Device d0r1z1-10.0.0.51:6201R10.0.0.51:6201/sdb_"" with 100.0 weight got id 0
	# swift-ring-builder container.builder add \
	  --region 1 --zone 1 --ip 10.0.0.51 --port 6201 --device sdc --weight 100
	Device d1r1z2-10.0.0.51:6201R10.0.0.51:6201/sdc_"" with 100.0 weight got id 1
	# swift-ring-builder container.builder add \
	  --region 1 --zone 2 --ip 10.0.0.52 --port 6201 --device sdb --weight 100
	Device d2r1z3-10.0.0.52:6201R10.0.0.52:6201/sdb_"" with 100.0 weight got id 2
	# swift-ring-builder container.builder add \
	  --region 1 --zone 2 --ip 10.0.0.52 --port 6201 --device sdc --weight 100
	Device d3r1z4-10.0.0.52:6201R10.0.0.52:6201/sdc_"" with 100.0 weight got id 3

Verify the ring contents:

#
 swift-ring-builder container.builder
Rebalance the ring:

# swift-ring-builder container.builder rebalance

cd /etc/swift

# swift-ring-builder object.builder create 10 3 1

# swift-ring-builder object.builder add --region 1 --zone 1 --ip 192.168.8.14 --port 6200 --device sdb --weight 100
# swift-ring-builder object.builder add --region 1 --zone 1 --ip 192.168.8.14 --port 6200 --device sdc --weight 100

备注: swift-ring-builder object.builder add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6200 --device DEVICE_NAME --weight DEVICE_WEIGHT
备注: Replace STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS with the IP address of the management network on the storage node. Replace DEVICE_NAME with a storage device name on the same storage node.  
备注示例:
	# swift-ring-builder object.builder add \
	  --region 1 --zone 1 --ip 10.0.0.51 --port 6200 --device sdb --weight 100
	Device d0r1z1-10.0.0.51:6200R10.0.0.51:6200/sdb_"" with 100.0 weight got id 0
	# swift-ring-builder object.builder add \
	  --region 1 --zone 1 --ip 10.0.0.51 --port 6200 --device sdc --weight 100
	Device d1r1z2-10.0.0.51:6200R10.0.0.51:6200/sdc_"" with 100.0 weight got id 1
	# swift-ring-builder object.builder add \
	  --region 1 --zone 2 --ip 10.0.0.52 --port 6200 --device sdb --weight 100
	Device d2r1z3-10.0.0.52:6200R10.0.0.52:6200/sdb_"" with 100.0 weight got id 2
	# swift-ring-builder object.builder add \
	  --region 1 --zone 2 --ip 10.0.0.52 --port 6200 --device sdc --weight 100
	Device d3r1z4-10.0.0.52:6200R10.0.0.52:6200/sdc_"" with 100.0 weight got id 3
	
Verify the ring contents:

# swift-ring-builder object.builder

Rebalance the ring:

# swift-ring-builder object.builder rebalance

备注:
	Distribute ring configuration files:
	Copy the account.ring.gz, container.ring.gz, and object.ring.gz files to the /etc/swift directory on each storage node and any additional nodes running the proxy service.


*************************************************************************************

Finalize installation:

1.Obtain the /etc/swift/swift.conf file from the Object Storage source repository:

# curl -o /etc/swift/swift.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/queens

2.Edit the /etc/swift/swift.conf:

[swift-hash]
swift_hash_path_suffix = iforyou
swift_hash_path_prefix = itoyou

[storage-policy:0]
name = Policy-0
default = yes

3.Copy the swift.conf file to the /etc/swift directory on each storage node and any additional nodes running the proxy service.

4.On all nodes, ensure proper ownership of the configuration directory:

# chown -R root:swift /etc/swift

5.On the controller node and any other nodes running the proxy service, start the Object Storage proxy service including its dependencies and configure them to start when the system boots:

# systemctl enable openstack-swift-proxy.service memcached.service
# systemctl start openstack-swift-proxy.service memcached.service

6.On the storage nodes, start the Object Storage services and configure them to start when the system boots:

# systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.service
# systemctl start openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.service
# systemctl enable openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.service
# systemctl start openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.service
# systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.service
# systemctl start openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.service



Verify operation:

Note: Perform these steps on the controller node.

备注: If you are using Red Hat Enterprise Linux 7 or CentOS 7 and one or more of these steps do not work, check the /var/log/audit/audit.log file for SELinux messages indicating denial of actions for the swift processes. If present, change the security context of the /srv/node directory to the lowest security level (s0) for the swift_data_t type, object_r role and the system_u user:

# chcon -R system_u:object_r:swift_data_t:s0 /srv/node

$ . /etc/demo-openrc

Show the service status:
$ swift stat

Create container1 container:
$ openstack container create container1

Upload a test file to the container1 container:
$ openstack object create container1 FILE

List files in the container1 container:
$ openstack object list container1

Download a test file from the container1 container:
$ openstack object save container1 FILE

备注: Replace FILE with the name of the file uploaded to the container1 container.


Object Storage service结束





























































